{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Christian Hellum Bye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a Pulsar Star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classification problem where the goal is to predict if some radio signal is due to a pulsar star or Radio Frequency Interference (RFI), i.e. noise. Each signal has eight features and one class. The features are: \"Mean of the integrated profile\", \"Standard deviation of the integrated profile\", \"Excess kurtosis of the integrated profile\", \"Skewness of the integrated profile\", \"Mean of the DM-SNR curve\", \"Standard deviation of the DM-SNR curve\", \"Excess kurtosis of the DM-SNR curve\" and \"Skewness of the DM-SNR curve\". The classes are 0 (RFI) and 1 (pulsar star)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is taken from https://www.kaggle.com/pavanraj159/predicting-a-pulsar-star. No changes will be made to the original data set as it is already clean and prepared for analysis. The dataset contains 17 898 total samples of which 1639 are positive (pulsar stars) and 16 259 are negative (RFI). Each feature is a decimal number where the number of decimal points in any given feature is consistent between different samples. Moreover, there are no missing datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No model was chosen in deliverable 1, but based on the classification methods discussed in lecture, we will try using Support Vector Machine (SVM). Given only eight features, it seems unnecessary to perform Principal Component Analysis (PCA) or reduce the amnount of data in other ways. Moreover, the Naive Bayes assumption might not apply in this case as we do not expect the features to be mutually independent. The k-NN method was considered and might be used, but it is unsure how much the curse of dimensionality will apply. Thus, SVM was chosen (for now, it might change as we develop more tools)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset will be split like the following: 60 % training, 20 % validation and 20 % test. Using a Linear Suppost Vector Classification Method, we would like to determine the following three hyperparameters: the penalty norm (L1 or L2 regularization), the regularization parameter (the coefficient C that sets the amount of regularization, i.e. determines if the margin is soft or hard) and the loss function - whether to use hinge loss or squared hinge loss. Given three hyperparameters, we have to make sure to have a large enough validation set in order to determine all three. This motivates the splitting ratio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first guess for the hyperparamters would be:\n",
    "\n",
    "- Using L2 regularization. This is because L1 penalty will make the coefficients of the less important feautures go to 0, effecitively reducing the number of features. With only eigth feautures, this is probably neiher necessary nor desirable as we expect all features to be relevant a priori.\n",
    "\n",
    "- Setting the regularization parameter to a relatively large number. As an initial test, we would like to check the overlap between the class distributions. A quick way to test this is by making the regularization parameter large, hence making a hard margin. If the accuracy on the validation set is close to that of the training set - i.e. low variance - in this limit, there is little overlap between class distributions.\n",
    "\n",
    "- Use squared hinge loss function. This penalizes large errors more than the hinge loss function, making the decision boundary to finer. This serves the same purpose as chosing a large regularization parameter.\n",
    "\n",
    "Of course, these hyperparameters needs to be tested for with the validation set, but the choice listed above represent an initial hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
