{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Christian Hellum Bye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting a Pulsar Star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classification problem where the goal is to predict if some radio signal is due to a pulsar star or Radio Frequency Interference (RFI), i.e. noise. Each signal has eight features and one class. The features are: \"Mean of the integrated profile\", \"Standard deviation of the integrated profile\", \"Excess kurtosis of the integrated profile\", \"Skewness of the integrated profile\", \"Mean of the DM-SNR curve\", \"Standard deviation of the DM-SNR curve\", \"Excess kurtosis of the DM-SNR curve\" and \"Skewness of the DM-SNR curve\". The classes are 0 (RFI) and 1 (pulsar star)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is taken from https://www.kaggle.com/pavanraj159/predicting-a-pulsar-star. No changes will be made to the original data set as it is already clean and prepared for analysis. The dataset contains 17 898 total samples of which 1639 are positive (pulsar stars) and 16 259 are negative (RFI). Each feature is a decimal number where the number of decimal points in any given feature is consistent between different samples. Moreover, there are no missing datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No model was chosen in deliverable 1, but based on the classification methods discussed in lecture, we will try using Support Vector Machine (SVM). Given only eight features, it seems unnecessary to perform Principal Component Analysis (PCA) or reduce the amnount of data in other ways. Moreover, the Naive Bayes assumption might not apply in this case as we do not expect the features to be mutually independent. The k-NN method was considered and might be used, but it is unsure how much the curse of dimensionality will apply. Thus, SVM was chosen (for now, it might change as we develop more tools)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset will be split like the following: 60 % training, 20 % validation and 20 % test. Using a Linear Suppost Vector Classification Method, we would like to determine the following three hyperparameters: the penalty norm (L1 or L2 regularization), the regularization parameter (the coefficient C that sets the amount of regularization, i.e. determines if the margin is soft or hard) and the loss function - whether to use hinge loss or squared hinge loss. Given three hyperparameters, we have to make sure to have a large enough validation set in order to determine all three. This motivates the splitting ratio. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first guess for the hyperparamters would be:\n",
    "\n",
    "- Using L2 regularization. This is because L1 penalty will make the coefficients of the less important feautures go to 0, effecitively reducing the number of features. With only eigth feautures, this is probably neiher necessary nor desirable as we expect all features to be relevant a priori.\n",
    "\n",
    "- Setting the regularization parameter to a relatively large number. As an initial test, we would like to check the overlap between the class distributions. A quick way to test this is by making the regularization parameter large, hence making a hard margin. If the accuracy on the validation set is close to that of the training set - i.e. low variance - in this limit, there is little overlap between class distributions.\n",
    "\n",
    "- Use squared hinge loss function. This penalizes large errors more than the hinge loss function, making the decision boundary to finer. This serves the same purpose as chosing a large regularization parameter.\n",
    "\n",
    "Of course, these hyperparameters needs to be tested for with the validation set, but the choice listed above represent an initial hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preliminary results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC #the training algorithm\n",
    "from sklearn.model_selection import train_test_split #to split the dataset\n",
    "from sklearn.metrics import confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('pulsar_stars.csv', delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, 0:8] #features\n",
    "y = data[:, 8] #classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into two parts, 80 % containing training and validation sets, 20 % to the test set\n",
    "X_train_validation, X_test, y_train_validation, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "#split the larger part of the dataset to two parts: 75 % (= 60 % of the total data) to training set, 25 % (= 20 % of the total)\n",
    "#to the validation set\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train_validation, y_train_validation, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets the hyperparamters as explained in section 3, with class_weight='balanced' to adjust the weights of each class to be\n",
    "#inversely proportional to the class frequency\n",
    "svm_clf = LinearSVC(penalty='l2', loss='squared_hinge', C=100, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=100, class_weight='balanced', dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(X_train, y_train) #fits the model to the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a large imbalance between postive and negative samples, so accuracy is not the best way to evaluate our model. Instead, we will use the F1-score, which works well to evaluate the balance between precision (how many of the precited positives are true positives) and recall (how many of the true positives are predicted to be positive). Moreover, F1-score takes into account that there is an uneven distribution between number of positive and negative samples. The formula for F1 is:\n",
    "\n",
    "$$ F_1 = 2 \\frac{Precision \\times Recall}{Precision + Recall}, $$\n",
    "\n",
    "where recall and precision are defined like this:\n",
    "\n",
    "$$ Recall = \\frac{True Positives}{True Positives + False Negatives} $$\n",
    "$$ Precision = \\frac{True Positives}{True Positives + False Positives} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the class labels for the training set and validation set\n",
    "train_predict = svm_clf.predict(X_train)\n",
    "validation_predict = svm_clf.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the f1-scores for the training and validation sets\n",
    "f1_train = f1_score(y_train, train_predict)\n",
    "f1_validation = f1_score(y_validation, validation_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1-scores are: \n",
      "\n",
      "Training set: 0.8785145888594165\n",
      "Validation set: 0.8892561983471075\n"
     ]
    }
   ],
   "source": [
    "print('The f1-scores are: \\n')\n",
    "print('Training set:', f1_train)\n",
    "print('Validation set:', f1_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the f1-scores that the validation set fits the data better than the training set so we are at least not overfitting. A first trials, the hyperparameters seem reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute the confusion matrix for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = svm_clf.predict(X_test) #the predicted classes for the test data\n",
    "confusion_test = confusion_matrix(y_test, test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "\n",
      "[[3197   32]\n",
      " [  59  292]]\n",
      "\n",
      "True positives 292\n",
      "True negatives 3197\n",
      "False positives 32\n",
      "False negatives 59\n"
     ]
    }
   ],
   "source": [
    "print('Confusion matrix: \\n')\n",
    "print(confusion_test)\n",
    "\n",
    "tn, fp, fn, tp = confusion_test.ravel()\n",
    "\n",
    "print('\\nTrue positives', tp)\n",
    "print('True negatives', tn)\n",
    "print('False positives', fp)\n",
    "print('False negatives', fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1-score for the test set is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score for test set: 0.8651851851851853\n"
     ]
    }
   ],
   "source": [
    "print('F1-score for test set:', f1_score(y_test, test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the F1-scores for the three parts of the datasets are very similar. As a first test, these F1-scores are acceptable. The greatest value possible for F1 is 1 and it seems feasible to push these results closer to the the theoretical upper bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going forward, we would like to tune the hyperparameters to see if its possible to get a better F1-score. If that is resultless, we might also want to try different models, for example k-NN or naive Bayes just to see how they perform in comparison to SVM. It should be fairly straight forward to implement other algoithms and try out different models, as the data is clean and easy to handle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
