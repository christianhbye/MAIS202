{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Christian Hellum Bye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting a Pulsar Star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on previous work, we have trained a Random Forest Classifier (RFC) as our model. In this deliverable, we explore the relative importance of the features and see if we can further improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split #to split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('../pulsar_stars.csv', delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, 0:8] #features\n",
    "y = data[:, 8] #classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into two parts, 80 % containing training and validation sets, 20 % to the test set\n",
    "X_train_validation, X_test, y_train_validation, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "#split the larger part of the dataset to two parts: 75 % (= 60 % of the total data) to training set, 25 % (= 20 % of the total)\n",
    "#to the validation set\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train_validation, y_train_validation, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = pickle.load(open('RFC_weigths.sav', 'rb')) #load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature labels\n",
    "feat_labels = ['Mean of the integrated profile', 'Standard deviation of the integrated profile', 'Excess kurtosis of the integrated profile', 'Skewness of the integrated profile', 'Mean of the DM-SNR curve', 'Standard deviation of the DM-SNR curve', 'Excess kurtosis of the DM-SNR curve', 'Skewness of the DM-SNR curve']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mean of the integrated profile', 0.246078065428781)\n",
      "('Standard deviation of the integrated profile', 0.026256904658293712)\n",
      "('Excess kurtosis of the integrated profile', 0.389001337277414)\n",
      "('Skewness of the integrated profile', 0.15003699764332742)\n",
      "('Mean of the DM-SNR curve', 0.0442104741037564)\n",
      "('Standard deviation of the DM-SNR curve', 0.10189573846224295)\n",
      "('Excess kurtosis of the DM-SNR curve', 0.01988925040409805)\n",
      "('Skewness of the DM-SNR curve', 0.02263123202208658)\n"
     ]
    }
   ],
   "source": [
    "for feature in zip(feat_labels, rfc.feature_importances_):\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that four of the features have importance scores of less than 10 %, whereas the other four account for over 85 % of the importance in total. We will train a model based on the four important features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_cols = [0, 2, 3, 5] #the coloumns in the dataset corresponding to the important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_important = X_train[:, important_cols]\n",
    "X_validation_important = X_validation[:, important_cols]\n",
    "X_test_important = X_test[:, important_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed the same way as in the previous deliverable to set the hyperparameters (all code in this part is copied from deliverable 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc_f1(n_estimators, min_samples_split, max_features):\n",
    "    rfc_important = RandomForestClassifier(n_estimators = n_estimators, min_samples_split = min_samples_split, max_features = max_features, class_weight='balanced')\n",
    "    rfc_important.fit(X_train_important, y_train) #fits to training set\n",
    "    \n",
    "    #make predictions\n",
    "    train_predict = rfc_important.predict(X_train_important)\n",
    "    validation_predict = rfc_important.predict(X_validation_important)\n",
    "    \n",
    "    rfc_tr_f1 = f1_score(y_train, train_predict) #f1-score for training data\n",
    "    rfc_validation_f1 = f1_score(y_validation, validation_predict) #f1-score for test data\n",
    "    \n",
    "    return rfc_tr_f1, rfc_validation_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vals = np.arange(1,51) #n_estimators\n",
    "min_sample_split_vals = np.arange(2,11) #min_sample_split\n",
    "max_features_vals = np.arange(1,5) #max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_rfc_train = np.empty((50, 9, 4))\n",
    "f1_scores_rfc_validation = np.empty((50, 9, 4))\n",
    "for i in range(50): #loop through values of n_estimators\n",
    "    for j in range(9): #loop through values of min_sample_split\n",
    "        for k in range(4): #loop through values of max_features\n",
    "            f1_scores_rfc_train[i,j,k], f1_scores_rfc_validation[i,j,k] = rfc_f1(n_vals[i], min_sample_split_vals[j], max_features_vals[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the arrays\n",
    "f1_tr = np.ravel(f1_scores_rfc_train)\n",
    "f1_val = np.ravel(f1_scores_rfc_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max f1-score for validation data set is: 0.900489396411093\n",
      "The index that maximizes the f1-score is: (11, 3, 0)\n"
     ]
    }
   ],
   "source": [
    "max_index_rfc = np.argmax(f1_val) #the index corresponding to the greatest f1-score for the validation data\n",
    "print('Max f1-score for validation data set is:', f1_val[max_index_rfc]) #the f1-score at this index\n",
    "print('The index that maximizes the f1-score is:', np.unravel_index(max_index_rfc, f1_scores_rfc_validation.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters are:\n",
    "* n_estimators = 10 (the 0th index has n_estimators = 1)\n",
    "* min_sample_split = 5 (the 0th index has min_sample_split = 2, so the third has min_sample_split = 5)\n",
    "* max_features = 1 (the 0th index has max_features = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_important = RandomForestClassifier(n_estimators = , min_samples_split = , max_features = , class_weight='balanced')\n",
    "rfc_important.fit(X_train_important, y_train) #fits to training set\n",
    "    \n",
    "test_predict = rfc_important.predict(X_test_important) #make predictions\n",
    "rfc_important_test_f1 = f1_score(y_test, test_predict) #f1-score for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix for our model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "\n",
      "[[3238   21]\n",
      " [  40  281]]\n",
      "\n",
      "True positives 281\n",
      "True negatives 3238\n",
      "False positives 21\n",
      "False negatives 40\n"
     ]
    }
   ],
   "source": [
    "confusion_test = confusion_matrix(y_test, test_predict)\n",
    "print('Confusion matrix: \\n')\n",
    "print(confusion_test)\n",
    "\n",
    "tn, fp, fn, tp = confusion_test.ravel()\n",
    "\n",
    "print('\\nTrue positives', tp)\n",
    "print('True negatives', tn)\n",
    "print('False positives', fp)\n",
    "print('False negatives', fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results will be presented with a poster. As a first draft, the poster will include the following:\n",
    "\n",
    "* Problem statement: similar to what's included in deliverable 2\n",
    "* Background: motivation for choosing the project, what are pulsars and why does the problem exist\n",
    "* The data used: description of the data, similar to what's in deliverable 1 and 2\n",
    "* Methodology: similar to what's in deliverable 1, but also including the process of selecting the final model as descriped in this deliverable\n",
    "* Results: the f1-scores and confusion matrix from this deliverable\n",
    "* Discussion: compare the results to baseline performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
